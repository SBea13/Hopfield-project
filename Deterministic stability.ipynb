{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hopfield model project\n",
    "Artificial Neural Networks (ANN) are computational techniques that aim to realize a very simplified model of the human brain. In this way, ANN try to learn tasks mimicking the behavior of brain. The brain is composed of a large set of elements, specialized cells called neurons. In analogy with the human brain, ANN are computational methods that use a large set of elementary computational: the neurons. Each single neuron is a very simple entity but the power of the brain is given by the fact that neurons are numerous and strongly interconnected between them. The human brain is one of the most computationally efficient device that is why in the late years a lot of effort has been done in order to develop an artificial version of it, as a matter of fact companies like Telsa motors are developing self driving cars which are based on ANN to implement the behavior of the human brain in computer systems. \n",
    "\n",
    "The easiest way to represent a neuron is either (on or off) or spin up/down in physics) with the synapsys either eccitative or inibitive. We can easily map the neural network into a spin system by mapping neurons in spins $\\{s_i\\}_{i=1,\\cdots,N}$ and synapsys into the magnetic coupling $J_{ij}$.\n",
    "By a specific representation of these coupling costant we can define the Hopfield model.\n",
    "\n",
    "## The Hopfield Model\n",
    "The Hopfield model is a fully connected neural network able to recall stored memories starting from a noisy or distorted input. The Hopfield network consists of N neurons connected through symmetric bidirectional links.The neuron interactions are encoded in the connection matrix, a N × N real symmetric matrix without self-interaction terms whose $J_{ij}$ entries define the weight of the connection between neuron i and j. \n",
    "\n",
    "The model respect the following mapping:\n",
    "\n",
    "$$\n",
    "n = \\{0,1\\} \\longrightarrow s = \\{-1,1\\}      \\mbox {     where n  is the binary operator and s the spin}\n",
    "$$\n",
    "                                   \n",
    "\n",
    "The correct transformation of this mapping is:\n",
    "$$\n",
    "\\begin{equation}\n",
    "s = f(n) = \\frac{2n-1}{2}\n",
    "\\end{equation}\n",
    "$$\n",
    "We define the synapses as:\n",
    "$$\n",
    "J_{ij}=\\begin{cases}\n",
    "+1 & \\mbox{ excitatory synapses} \\\\\n",
    "-1 & \\mbox{ inhibitory synapses}\n",
    "\\end{cases}\n",
    "$$\n",
    "\n",
    "A neuron is activated if it receives a sufficient number of active impulsive, and we can compute these impulses as follows:\n",
    "$$\n",
    "\\begin{equation}\n",
    "h_i(t) = \\sum^N_{j=1,j\\neq i} J_{ij}(s_j(t)+1)\n",
    "\\label{eq:h} \n",
    "\\end{equation}\n",
    "$$\n",
    "It is important to notice that $j\\neq i$ since the neuron DOES NOT interacts with itself.\n",
    "To decide if these impulses are sufficient to activate the neuron we apply a non-linear function to the impulses:\n",
    "$$\n",
    "\\begin{equation}\n",
    "s_i(t+1)=sign{\\left(h_i(t)-\\theta_i\\right)}\n",
    "\\end{equation}\n",
    "$$\n",
    "where $\\theta_i$ is the threshold. We choose this threshold in a way that is useful for the calculations:\n",
    "$$\n",
    "\\theta_i = \\sum^N_{j=1,j\\neq i} J_{ij}.\n",
    "$$\n",
    "By applying this threshold we obtain in Equation $\\eqref{eq:upd}$ $\\textbf{update rule}$ that we will use below in the code:\n",
    "$$\n",
    "\\begin{equation}\n",
    "s_i(t+1)=sign{\\left(\\sum^N_{j=1,j\\neq i} J_{ij}s_j(t)\\right)}\n",
    "\\label{eq:upd}\n",
    "\\end{equation}\n",
    "$$\n",
    "\n",
    "The $\\textbf{Hopfield model}$ that we will use in the following consists in a specific choice of the synapses (ferromagnetic couplings):\n",
    "$$\n",
    "\\\n",
    "\\begin{equation}\n",
    "\\begin{cases}\n",
    "J_{ii}=0 & \\mbox{ known as Hebb rule} \\\\\n",
    "J_{ij}=\\frac{1}{N}\\sum_{\\mu=1}^{p} \\xi^\\mu_i\\xi^\\mu_j\n",
    "\\end{cases}\n",
    "\\label{eq:hop}\n",
    "\\end{equation}\n",
    "$$\n",
    "where the $\\vec{\\xi^\\mu}$ are $p<<N$ excitatory pattern with $\\xi^\\mu_i=\\{+1,-1\\}$.\n",
    "This choice encode these patterns in the couplings and gives to the system some interesting properties:\n",
    "- if the system start from a configuration equal to a pattern $\\vec{\\xi^\\mu}$ and apply the update rule of Eq $\\eqref{eq:upd}$ it stays in that pattern $\\forall t$.\n",
    "- by solving the system from a statistical mechanics point of view turns out that the system has many minima and this minima are all and only the patterns.\n",
    "\n",
    "The actual resolution of this system is quite long and it is not the aim of this work, but it is instructive to see the proof of the stability of the patterns.\n",
    "$$\n",
    "s(1)=sign{\\left(\\sum^N_{j=1} J_{ij}s_j(0)\\right)}\\overset{s_j(0)=\\xi^\\mu_j}{=}\n",
    "sign{\\left(\\sum^N_{j=1} \\frac{1}{N}\\sum^p_{\\nu=1}\\xi_i^\\nu\\xi_j^\\nu\\xi_j^\\mu\\right)} \n",
    "\\\\=sign{\\left(\\sum^N_{\\nu=1}\\xi_i^\\nu\\frac{1}{N}\\sum^p_{j=1}\\xi_j^\\nu\\xi_j^\\mu \\right)} \n",
    "=sign{\\left(\\sum^N_{\\nu=1}\\xi_i^\\nu(\\delta_{\\mu\\nu}+O(N^{-\\frac{1}{2}}) \\right)}\n",
    "\\simeq sign\\left(\\xi^\\mu_i \\right)=\\xi^\\mu_i\n",
    "$$\n",
    "And so the property enuciated before holds.\n",
    "\n",
    "From now on we will talk about spins and not neurons anymore.\n",
    "\n",
    "This model take into account fully connected system, where each spin is connected to all the others. But what happen if we consider an interaction lenght $R$? It is an interesting question to look at how the results vary with this assumption.\n",
    "\n",
    "It is also really interesting to notice that this type of system does not need training, and so it can be reallly interesting if the results are good."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy as scp\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameters\n",
    "L = 11  # Lenght of the edge of the image (change to 11?)\n",
    "N = L**2 # Number of spins \n",
    "p = 10 # Number of patterns\n",
    "MF = 0 # Use or not the Mean Field strategy: if MF=1 uses MF, if MF = 0 only spins at\n",
    "# a distance R interacts\n",
    "R = 3 # The 3-nearest neighbor interacts\n",
    "np.random.seed(1234) # Seed to make the random process reproducible"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Patterns\n",
    "def random_patterns(N,p):\n",
    "    xi = np.random.randint(0,2,N*p).reshape(p,N) # Each line is a pattern\n",
    "    xi[xi==0]=-1\n",
    "    return xi\n",
    "\n",
    "xi = random_patterns(N,p)\n",
    "idx = np.random.randint(0,p)\n",
    "plt.imshow(xi[idx].reshape(L,L),cmap='Greys') # This is an example of pattern \n",
    "# -1 = white, +1 = black"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Coupling constant\n",
    "# Row i is the connections of the i-th neuron with the others.\n",
    "# Note that the hopfield model requires Jii=0\n",
    "# Note that J is a symmetrical matrix (J=J.T)\n",
    "def MF_coupling(xi,N):\n",
    "    J = 1/N * np.array( [ (xi[:,i]*xi[:,j]).sum() for i in range(N) for j in range(N) ] )\n",
    "    J = J.reshape(N,N)        \n",
    "    for i in range(len(J)):\n",
    "        J[i,i] = 0\n",
    "    return J\n",
    "def R_coupling(xi,N,R):\n",
    "    J = MF_coupling(xi,N)\n",
    "    for i in range( J.shape[0] ):\n",
    "        J_temp = J[i].reshape(L,L)\n",
    "        for j in range(L):\n",
    "            y = (i%L -j)**2 # Look at indexes should be ok -1\n",
    "            for k in range(L):\n",
    "                if np.sqrt( (i//L - k)**2 + y ) > R: J_temp[j,k] = 0\n",
    "        J[i] = J_temp.reshape(1,N)\n",
    "    return J"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "global J\n",
    "if MF: J = MF_coupling(xi,N)\n",
    "else: J = R_coupling(xi,N,R)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Update rule\n",
    "def update(sys):\n",
    "    N = len(sys)\n",
    "    for i in range(N):\n",
    "        temp = 0\n",
    "        for j in range(N): #Maybe it is possible to set the cycle on J[i][j]!=0 only\n",
    "            temp +=  J[i][j]*sys[j] \n",
    "        sys[i] = np.sign(temp)\n",
    "    return np.sign(sys)\n",
    "\n",
    "def deterministic_hopfield(sys, t):\n",
    "    for i in range(t):\n",
    "        sys = update(sys)\n",
    "    return sys\n",
    "\n",
    "def error_im(xi_idx, sys):\n",
    "    wrong_pixels = (np.abs( sys-xi_idx )/2).sum()\n",
    "    return wrong_pixels/len(sys)\n",
    "\n",
    "def assign_pattern(xi,sys):\n",
    "    errors = [ error_im(i,sys) for i in xi ]\n",
    "    correct = np.argmin(errors)\n",
    "    return correct\n",
    "\n",
    "def total_error(xi,t):\n",
    "    errors = []\n",
    "    prediction = []\n",
    "    for mu in range(len(xi)):\n",
    "        sys = deterministic_hopfield(xi[mu],t)\n",
    "        errors.append( error_im(xi[mu],sys) )\n",
    "        if assign_pattern(xi,sys)==mu:\n",
    "            prediction.append( 1 )\n",
    "        else: prediction.append( 0 )\n",
    "    errors = (np.array(errors)).mean()\n",
    "    prediction = np.array(prediction).sum()/len(xi)\n",
    "    return errors, prediction\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# actual algorithm\n",
    "from copy import deepcopy\n",
    "idx = np.random.randint(0,p)\n",
    "sys = deepcopy(xi[idx])\n",
    "\n",
    "sys = deterministic_hopfield(sys,100)\n",
    "wrong_pixels = error_im(xi[idx],sys)\n",
    "assigned_pattern = assign_pattern(xi,sys)\n",
    "\n",
    "fig, ax = plt.subplots(1,2, figsize = (10,6))\n",
    "ax[0].set_title('Pattern')\n",
    "ax[0].imshow(xi[idx].reshape(L,L), cmap='Greys')\n",
    "ax[1].set_title('Finishing configuration')\n",
    "ax[1].imshow(sys.reshape(L,L), cmap='Greys')\n",
    "print('The error of the algorithm is %f' %(wrong_pixels))\n",
    "print('The algorithm recognised the pattern %i and the correct pattern is %i' %(assigned_pattern,idx))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analisys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We now look at the scaling with regards to the parameters\n",
    "L = 16\n",
    "N = L**2\n",
    "\n",
    "P = np.arange(5,N/2,20)\n",
    "\n",
    "for MF in range(2):\n",
    "    scores = []\n",
    "    errors = []\n",
    "    for p in P: \n",
    "        p = int(p)\n",
    "        xi = random_patterns(N,p)\n",
    "        if MF: J = MF_coupling(xi,N)\n",
    "        else: J = R_coupling(xi,N,R)\n",
    "        err, pred = total_error(xi,30)    \n",
    "        scores.append( pred )\n",
    "        errors.append(err)\n",
    "    print(\"MF = %i\", MF)\n",
    "    figp, axp = plt.subplots(1,2, figsize=(14,6))\n",
    "    axp[0].set_title('Dependance of the error by number of pattern p')\n",
    "    axp[0].set_xlabel('Number of pattern p')\n",
    "    axp[0].set_ylabel('Average error along the p patterns')\n",
    "    axp[0].plot(P,errors, label='Error')\n",
    "    axp[0].legend()\n",
    "\n",
    "    axp[1].set_title('Dependance of the prediction of the correct pattern by number of pattern p')\n",
    "    axp[1].set_xlabel('Number of pattern p')\n",
    "    axp[1].set_ylabel('Fraction of pattern correctly predicted')\n",
    "    axp[1].plot(P,scores, label='Score')\n",
    "    axp[1].legend()    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dependance of error with patterns is a staight line for the first few patterns showing the linearity of the error with the number of patterns, it then becomes a sawtooth wave for the rest of the patterns proving the instability of the algorithm. The dependence of the prediction of the correct pattern with number of patterns is a straigh line meaning that the score is always constant whichever the pattern thus the independence of the error with  number of patterns. For the average error, it starts at a constant rate for the very first few patterns and grows linearly with some noise until it reaches its maxmum then it reduces at a linear rate for the last few pattens. The fraction of the correctly predicted patterns is maximum and constant for the first few patterns, it then decreases linearly with different slopes for the rest of the patterns implying that the fraction is propotional to a given number of patterns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "figp, axp = plt.subplots(1,2, figsize=(14,6))\n",
    "axp[0].set_title('Dependance of the error by number of pattern p')\n",
    "axp[0].set_xlabel('Number of pattern p')\n",
    "axp[0].set_ylabel('Average error along the p patterns')\n",
    "axp[0].plot(P,errors, label='Error')\n",
    "axp[0].legend()\n",
    "\n",
    "axp[1].set_title('Dependance of the prediction of the correct pattern by number of pattern p')\n",
    "axp[1].set_xlabel('Number of pattern p')\n",
    "axp[1].set_ylabel('Fraction of pattern correctly predicted')\n",
    "axp[1].plot(P,scores, label='Score')\n",
    "axp[1].legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Comment these results\n",
    "- optimal number of patterns <20, so % of system\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# How does the time scales with the number of spins N?\n",
    "import time as time\n",
    "L = np.arange(10,28,4)\n",
    "N = L**2 \n",
    "p = 10\n",
    "times = []\n",
    "for n in N:\n",
    "    xi = np.random.randint(0,2,n*p).reshape(p,n)\n",
    "    xi[xi==0]=-1\n",
    "    J = 1/n * np.array( [ (xi[:,i]*xi[:,j]).sum() for i in range(n) for j in range(n) ] )\n",
    "    J = J.reshape(n,n)\n",
    "    for k in range(len(J)):\n",
    "        J[k,k] = 0\n",
    "    idx = np.random.randint(0,p)\n",
    "    start = time.time()\n",
    "    deterministic_hopfield(xi[idx],30)\n",
    "    end = time.time()\n",
    "    times.append(end-start)\n",
    "\n",
    "fig, ax = plt.subplots(figsize = (8, 8))\n",
    "ax.plot(N,times, label = 'Data with 10 patterns')\n",
    "ax.set(xlabel = ' Number of spins N', ylabel =' Implementation time [s]', \n",
    "              title = '  Dependency of the time implementation on the size of the system')\n",
    "ax.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The variation of implementation time with the number of spins follows a linear form with different slopes. Implementation time increases as the number of spins increases, for N in (500, 700), the implementation time increases slowly meaning that more time is required when a number of spins are used. In general the implementation time grows exponentially with number of spins.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# How does the time scales with the number of pattern p with a fixed N?\n",
    "import time as time\n",
    "L = 16\n",
    "N = L**2 \n",
    "P = np.arange(10,N,16)\n",
    "times = []\n",
    "for p in P:\n",
    "    p = int(p)\n",
    "    xi = np.random.randint(0,2,N*p).reshape(p,N)\n",
    "    xi[xi==0]=-1\n",
    "    J = 1/N * np.array( [ (xi[:,i]*xi[:,j]).sum() for i in range(N) for j in range(N) ] )\n",
    "    J = J.reshape(N,N)\n",
    "    for k in range(len(J)):\n",
    "        J[k,k] = 0\n",
    "    idx = np.random.randint(0,p)\n",
    "    start = time.time()\n",
    "    deterministic_hopfield(xi[idx],30)\n",
    "    end = time.time()\n",
    "    times.append(end-start)\n",
    "\n",
    "fig, ax = plt.subplots(figsize = (10.5, 10))\n",
    "ax.plot(P,times, label = 'Data with 16x16 spins')\n",
    "ax.set(xlabel = 'Number of patterns p', ylabel =' Implementation time [s]', \n",
    "              title = ' Dependency of the time implementation on the number of patterns')\n",
    "ax.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
